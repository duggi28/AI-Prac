{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "6259803c-1aff-493d-bbe4-54fe31c10d90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fugashi[unidic-lite] in /Users/rithvikduggireddy/opt/anaconda3/envs/cs4701/lib/python3.11/site-packages (1.3.2)\n",
      "Requirement already satisfied: unidic-lite in /Users/rithvikduggireddy/opt/anaconda3/envs/cs4701/lib/python3.11/site-packages (from fugashi[unidic-lite]) (1.0.8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipadic in /Users/rithvikduggireddy/opt/anaconda3/envs/cs4701/lib/python3.11/site-packages (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q datasets jiwer\n",
    "!pip install fugashi[unidic-lite]\n",
    "!pip install ipadic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "03f1c373-9152-44eb-ab3d-4b85207cc6d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '0x3042', 1: '0x3044', 2: '0x3046', 3: '0x3048', 4: '0x304a', 5: '0x304b', 6: '0x304c', 7: '0x304d', 8: '0x304e', 9: '0x304f', 10: '0x3050', 11: '0x3051', 12: '0x3052', 13: '0x3053', 14: '0x3054', 15: '0x3055', 16: '0x3056', 17: '0x3057', 18: '0x3058', 19: '0x3059', 20: '0x305a', 21: '0x305b', 22: '0x305c', 23: '0x305d', 24: '0x305e', 25: '0x305f', 26: '0x3060', 27: '0x3061', 28: '0x3062', 29: '0x3064', 30: '0x3065', 31: '0x3066', 32: '0x3067', 33: '0x3068', 34: '0x3069', 35: '0x306a', 36: '0x306b', 37: '0x306c', 38: '0x306d', 39: '0x306e', 40: '0x306f', 41: '0x3070', 42: '0x3071', 43: '0x3072', 44: '0x3073', 45: '0x3074', 46: '0x3075', 47: '0x3076', 48: '0x3077', 49: '0x3078', 50: '0x3079', 51: '0x307a', 52: '0x307b', 53: '0x307c', 54: '0x307d', 55: '0x307e', 56: '0x307f', 57: '0x3080', 58: '0x3081', 59: '0x3082', 60: '0x3084', 61: '0x3086', 62: '0x3088', 63: '0x3089', 64: '0x308a', 65: '0x308b', 66: '0x308c', 67: '0x308d', 68: '0x308f', 69: '0x3092', 70: '0x3093'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.jpg</td>\n",
       "      <td>[53, 62, 2, 19, 25, 66, 40, 16, 46, 53]</td>\n",
       "      <td>ぼようすたれはざふぼ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.jpg</td>\n",
       "      <td>[6, 49, 22, 66, 51, 20, 22, 16, 59, 47]</td>\n",
       "      <td>がへぜれぺずぜざもぶ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.jpg</td>\n",
       "      <td>[44, 30, 24, 31, 15, 14, 9, 69, 25, 41]</td>\n",
       "      <td>びづぞてさごくをたば</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.jpg</td>\n",
       "      <td>[16, 38, 22, 18, 62, 29, 53, 45, 3, 35]</td>\n",
       "      <td>ざねぜじよつぼぴえな</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.jpg</td>\n",
       "      <td>[10, 59, 45, 35, 3, 4, 24, 0, 45, 15]</td>\n",
       "      <td>ぐもぴなえおぞあぴさ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     img                                    label        text\n",
       "0  0.jpg  [53, 62, 2, 19, 25, 66, 40, 16, 46, 53]  ぼようすたれはざふぼ\n",
       "1  1.jpg  [6, 49, 22, 66, 51, 20, 22, 16, 59, 47]  がへぜれぺずぜざもぶ\n",
       "2  2.jpg  [44, 30, 24, 31, 15, 14, 9, 69, 25, 41]  びづぞてさごくをたば\n",
       "3  3.jpg  [16, 38, 22, 18, 62, 29, 53, 45, 3, 35]  ざねぜじよつぼぴえな\n",
       "4  4.jpg    [10, 59, 45, 35, 3, 4, 24, 0, 45, 15]  ぐもぴなえおぞあぴさ"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "unicode_to_class={}\n",
    "class_to_unicode={}\n",
    "unicodes=[\"3042\", \"3044\", \"3046\", \"3048\", \"304a\", \"304b\", \"304c\", \"304d\", \"304e\", \"304f\", \"3050\", \"3051\", \"3052\", \"3053\", \"3054\", \"3055\", \"3056\", \"3057\", \"3058\", \"3059\", \"305a\", \"305b\", \"305c\", \"305d\", \"305e\", \"305f\", \"3060\", \"3061\", \"3062\", \"3064\", \"3065\", \"3066\", \"3067\", \"3068\", \"3069\", \"306a\", \"306b\", \"306c\", \"306d\", \"306e\", \"306f\", \"3070\", \"3071\", \"3072\", \"3073\", \"3074\", \"3075\", \"3076\", \"3077\", \"3078\", \"3079\", \"307a\", \"307b\", \"307c\", \"307d\", \"307e\", \"307f\", \"3080\", \"3081\", \"3082\", \"3084\", \"3086\", \"3088\", \"3089\", \"308a\", \"308b\", \"308c\", \"308d\", \"308f\", \"3092\", \"3093\"]\n",
    "count=0\n",
    "for u in unicodes:\n",
    "    hex=\"0x\"+u\n",
    "    unicode_to_class[hex]=count\n",
    "    class_to_unicode[count]=hex\n",
    "    count+=1\n",
    "print(class_to_unicode)\n",
    "\n",
    "def class2hiragana(class_sequence):\n",
    "    return \"\".join(list(map(lambda pred: chr(int(class_to_unicode[int(pred)], 16)), class_sequence)))\n",
    "def json_to_df_labels(file):\n",
    "    with open(file) as user_file:\n",
    "      file_contents = user_file.read()\n",
    "    sentence_unicode_labels = json.loads(file_contents)\n",
    "    sentence_class_labels = dict(map(lambda x: (x + '.jpg', list(map(lambda unicode: unicode_to_class[unicode],sentence_unicode_labels[x]))), sentence_unicode_labels))\n",
    "    return pd.DataFrame([{'img':img, 'label':list(map(lambda unicode: int(unicode), label))} for img, label in sentence_class_labels.items()])\n",
    "\n",
    "df = json_to_df_labels('sentence_labels.json')\n",
    "df['text']=df['label'].apply(lambda x: class2hiragana(x))\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "b8f22e68-c010-4c28-bb4a-43313dd50dea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2)\n",
    "# we reset the indices to start from zero\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "92a56708-f3a6-4e5b-a2e4-496cd17d2786",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# from transformers import TrOCRProcessor\n",
    "# processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "# image = Image.open(\"Sentences/0.jpg\").convert(\"RGB\")\n",
    "# processor(image, return_tensors=\"pt\").pixel_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "f708c6f9-2599-4082-995d-888153f519e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-char\" )\n",
    "# b = df['label'][10]\n",
    "# print(\"LABEL:\\n\",b)\n",
    "# line = class2hiragana(b)\n",
    "# print(\"HIRAGANA:\\n\",line,'\\n')\n",
    "# inputs = tokenizer(class2hiragana(b), return_tensors=\"pt\")\n",
    "# print('TOKENIZED:\\n',inputs)\n",
    "# print(\"DECODED:\\n\",tokenizer.decode(inputs[\"input_ids\"][0]))\n",
    "# inputs['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "e8a4a5cb-4419-45b6-a254-afdc424f1192",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# idx = 14\n",
    "# b = df['label'][idx]\n",
    "# print(class2hiragana(b))\n",
    "# labels = processor.tokenizer(class2hiragana(b), padding=\"max_length\", max_length=128).input_ids\n",
    "# print(labels)\n",
    "# print(processor.decode(labels, skip_special_tokens=True))\n",
    "# print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "e7c768bc-33e5-434e-b408-0e2dfd63f46d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# b = df['label'][8]\n",
    "# prompt = class2hiragana(b)\n",
    "\n",
    "# input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "# print(input_ids)\n",
    "# # gen_tokens = model.generate(\n",
    "# #     input_ids,\n",
    "# #     do_sample=True,\n",
    "# #     temperature=0.9,\n",
    "# #     max_length=100,\n",
    "# # )\n",
    "# gen_text = tokenizer.batch_decode(input_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "# print(prompt)\n",
    "# print(gen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "e4601efc-3094-4ae0-a32c-b6b1d91da645",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "まひぞすざつつぐへう\n",
      "まひぞすざつつぐへう\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPTNeoXJapaneseTokenizer.from_pretrained(\"abeja/gpt-neox-japanese-2.7b\")\n",
    "print(class2hiragana(b))\n",
    "inputs = tokenizer(class2hiragana(b), return_tensors=\"pt\", padding='max_length', max_length=128).input_ids\n",
    "print(tokenizer.batch_decode(inputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "0695c62f-6dfc-422a-879f-ca92be39d7fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import Resize\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, img_dir, df, processor, tokenizer, max_target_length=128):\n",
    "        self.img_dir = img_dir\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get file name + text \n",
    "        file_name = self.df['img'][idx]\n",
    "        text = self.df['text'][idx]\n",
    "        # prepare image (i.e. resize + normalize)\n",
    "        image = Image.open(os.path.join(self.img_dir, file_name)).convert(\"RGB\")\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
    "        # add labels (input_ids) by encoding the text\n",
    "        labels = self.tokenizer(text,\n",
    "                                          padding=\"max_length\", \n",
    "                                          max_length=self.max_target_length).input_ids\n",
    "        # important: make sure that PAD tokens are ignored by the loss function\n",
    "        labels = [label if label != self.tokenizer.pad_token_id else -100 for label in labels]\n",
    "\n",
    "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
    "                \n",
    "        return encoding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "9a03e350-5444-4101-aa25-39286fea94f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel, GPTNeoXJapaneseTokenizer\n",
    "import requests\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "tokenizer = GPTNeoXJapaneseTokenizer.from_pretrained(\"abeja/gpt-neox-japanese-2.7b\")\n",
    "train_dataset = CustomImageDataset(img_dir='Sentences/',\n",
    "                                   df = train_df,\n",
    "                                   processor=processor,\n",
    "                                   tokenizer=tokenizer)\n",
    "eval_dataset = CustomImageDataset(img_dir='Sentences/',\n",
    "                                   df = test_df,\n",
    "                                   processor=processor,\n",
    "                                   tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "6b5acd2c-c5f9-43dc-a7cb-c6ae6cb071f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 800\n",
      "Number of validation examples: 200\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of validation examples:\", len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "8616904f-1636-44cf-b1a2-f415edb2c436",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values torch.Size([3, 384, 384])\n",
      "labels torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "encoding = train_dataset[2]\n",
    "for k,v in encoding.items():\n",
    "  print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "c4f4d7b8-483a-4b9c-9a81-f1ab815d7b34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=300x48>"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = Image.open(train_dataset.img_dir + train_df['img'][2]).convert(\"RGB\")\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "2c872ded-2461-48f2-beea-50dc0801a6fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "つぞうえみもんよほで\n"
     ]
    }
   ],
   "source": [
    "labels = encoding['labels']\n",
    "labels[labels == -100] = tokenizer.pad_token_id\n",
    "label_str = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "print(label_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "29706655-6c58-4e35-ac3a-8a163967bd00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "e90bcdd1-e590-463d-b612-450f647144af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-384 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese and are newly initialized: ['bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionEncoderDecoderModel(\n",
       "  (encoder): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViTPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): BertLMHeadModel(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cls): BertOnlyMLMHead(\n",
       "      (predictions): BertLMPredictionHead(\n",
       "        (transform): BertPredictionHeadTransform(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (transform_act_fn): GELUActivation()\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): Linear(in_features=768, out_features=32000, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import VisionEncoderDecoderModel\n",
    "import torch\n",
    "import requests\n",
    "\n",
    "# initialize the encoder from a pretrained ViT and the decoder from a pretrained BERT model. \n",
    "# Note that the cross-attention layers will be randomly initialized, and need to be fine-tuned on a downstream dataset\n",
    "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    \"google/vit-base-patch16-384\", \"cl-tohoku/bert-base-japanese\"\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "500887a5-dd09-4f55-827f-954f28c2cf29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set special tokens used for creating the decoder_input_ids from the labels\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "# make sure vocab size is set correctly\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "# set beam search parameters\n",
    "model.config.eos_token_id = tokenizer.sep_token_id\n",
    "model.config.max_length = 128\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "287528c4-614b-46ce-abaf-a721da1d2315",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-char\")\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "825a61bc-2d6d-4c20-9a6f-2707d28f0612",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.decoder.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "8f5584a5-a960-4350-80ca-b0a46173528e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "cer_metric = load_metric(\"cer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "d57a7127-0e9e-4f51-9e7b-9255aafda49f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in /Users/rithvikduggireddy/opt/anaconda3/envs/cs4701/lib/python3.11/site-packages (8.1.2)\n",
      "Requirement already satisfied: comm>=0.1.3 in /Users/rithvikduggireddy/opt/anaconda3/envs/cs4701/lib/python3.11/site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Users/rithvikduggireddy/opt/anaconda3/envs/cs4701/lib/python3.11/site-packages (from ipywidgets) (8.20.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/rithvikduggireddy/opt/anaconda3/envs/cs4701/lib/python3.11/site-packages (from ipywidgets) (5.7.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in /Users/rithvikduggireddy/opt/anaconda3/envs/cs4701/lib/python3.11/site-packages (from ipywidgets) (4.0.10)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in /Users/rithvikduggireddy/opt/anaconda3/envs/cs4701/lib/python3.11/site-packages (from ipywidgets) (3.0.10)\n",
      "Requirement already satisfied: decorator in /Users/rithvikduggireddy/opt/anaconda3/envs/cs4701/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/rithvikduggireddy/opt/anaconda3/envs/cs4701/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/rithvikduggireddy/opt/anaconda3/envs/cs4701/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /Users/rithvikduggireddy/opt/anaconda3/envs/cs4701/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/rithvikduggireddy/opt/anaconda3/envs/cs4701/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /Users/rithvikduggireddy/opt/anaconda3/envs/cs4701/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/rithvikduggireddy/opt/anaconda3/envs/cs4701/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /Users/rithvikduggireddy/opt/anaconda3/envs/cs4701/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/rithvikduggireddy/opt/anaconda3/envs/cs4701/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/rithvikduggireddy/opt/anaconda3/envs/cs4701/lib/python3.11/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: executing in /Users/rithvikduggireddy/opt/anaconda3/envs/cs4701/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /Users/rithvikduggireddy/opt/anaconda3/envs/cs4701/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /Users/rithvikduggireddy/opt/anaconda3/envs/cs4701/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six in /Users/rithvikduggireddy/opt/anaconda3/envs/cs4701/lib/python3.11/site-packages (from asttokens->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in /Users/rithvikduggireddy/opt/anaconda3/envs/cs4701/lib/python3.11/site-packages (8.1.2)\n",
      "Requirement already satisfied: comm>=0.1.3 in /Users/rithvikduggireddy/opt/anaconda3/envs/cs4701/lib/python3.11/site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Users/rithvikduggireddy/opt/anaconda3/envs/cs4701/lib/python3.11/site-packages (from ipywidgets) (8.20.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/rithvikduggireddy/opt/anaconda3/envs/cs4701/lib/python3.11/site-packages (from ipywidgets) (5.7.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in /Users/rithvikduggireddy/opt/anaconda3/envs/cs4701/lib/python3.11/site-packages (from ipywidgets) (4.0.10)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in /Users/rithvikduggireddy/opt/anaconda3/envs/cs4701/lib/python3.11/site-packages (from ipywidgets) (3.0.10)\n",
      "Requirement already satisfied: decorator in /Users/rithvikduggireddy/opt/anaconda3/envs/cs4701/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/rithvikduggireddy/opt/anaconda3/envs/cs4701/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/rithvikduggireddy/opt/anaconda3/envs/cs4701/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /Users/rithvikduggireddy/opt/anaconda3/envs/cs4701/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/rithvikduggireddy/opt/anaconda3/envs/cs4701/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /Users/rithvikduggireddy/opt/anaconda3/envs/cs4701/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/rithvikduggireddy/opt/anaconda3/envs/cs4701/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /Users/rithvikduggireddy/opt/anaconda3/envs/cs4701/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/rithvikduggireddy/opt/anaconda3/envs/cs4701/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/rithvikduggireddy/opt/anaconda3/envs/cs4701/lib/python3.11/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: executing in /Users/rithvikduggireddy/opt/anaconda3/envs/cs4701/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /Users/rithvikduggireddy/opt/anaconda3/envs/cs4701/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /Users/rithvikduggireddy/opt/anaconda3/envs/cs4701/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six in /Users/rithvikduggireddy/opt/anaconda3/envs/cs4701/lib/python3.11/site-packages (from asttokens->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "495bc5f959284bdabdf2e097f22f8f1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n",
      "next batch...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[309], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m---> 20\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs4701/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs4701/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    267\u001b[0m     tensors,\n\u001b[1;32m    268\u001b[0m     grad_tensors_,\n\u001b[1;32m    269\u001b[0m     retain_graph,\n\u001b[1;32m    270\u001b[0m     create_graph,\n\u001b[1;32m    271\u001b[0m     inputs,\n\u001b[1;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    274\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "!pip install ipywidgets\n",
    "!pip install ipywidgets\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "   # train\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch in tqdm(train_dataloader, total=len(train_dataloader)):\n",
    "        print(\"next batch...\")\n",
    "        # get the inputs\n",
    "        for k,v in batch.items():\n",
    "            batch[k] = v\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    print(f\"Loss after epoch {epoch}:\", train_loss/len(train_dataloader))\n",
    "    \n",
    "    # evaluate\n",
    "    model.eval()\n",
    "    valid_cer = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(eval_dataloader):\n",
    "            # run batch generation\n",
    "            outputs = model.generate(batch[\"pixel_values\"])\n",
    "            # compute metrics\n",
    "            cer = compute_cer(pred_ids=outputs, label_ids=batch[\"labels\"])\n",
    "            valid_cer += cer \n",
    "\n",
    "    print(\"Validation CER:\", valid_cer / len(eval_dataloader))\n",
    "\n",
    "model.save_pretrained(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "a4693e35-8fc5-4527-ae06-244717b274ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save({\n",
    "                                'model_state_dict': model.state_dict(),\n",
    "                                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                                }, 'trocr.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "4aa9fbac-b8b6-478f-b5a6-05ce55009293",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load('trocr.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "01c67027-3774-4e6b-ad76-94cd668129bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = next(iter(eval_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "ef7360f0-a3f2-4d37-8ecc-b1c4c69ac8b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a_outputs = model.generate(a['pixel_values'][:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "25776ad5-e887-4ac5-a0ca-e889a4c76c95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a_labels = a['labels'][:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "dab7e18d-1294-4c16-9119-981089db54f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "せねむずるこぺごびな\n"
     ]
    }
   ],
   "source": [
    "labels = a_labels\n",
    "labels[labels == -100] = tokenizer.pad_token_id\n",
    "label_str = tokenizer.decode(labels.squeeze(), skip_special_tokens=True)\n",
    "print(label_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "f5065392-ea5e-4819-a4d5-c3dd5badd742",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "してずずずぞずぞぞぞずぐずずぐぞずずぱずぞぐぞぞぐずぐぐずぞぱずずぷぞずぱぞぞぱぞずぷずぞぷずずじぞずじずぞじぞぞぷぞぞじずぐぱぞぐぐぐぞぱぐずぱぐぞぐぷずぐぷぞぐぱずぐじぞぐじずずべずぞべぞずべぞぞべずずねずぞねずずんずぞんずずよずぞよぞずねぞぞねぞずんぞ\n"
     ]
    }
   ],
   "source": [
    "preds = a_outputs\n",
    "preds[preds == -100] = tokenizer.pad_token_id\n",
    "preds_str = tokenizer.decode(preds.squeeze(), skip_special_tokens=True)\n",
    "print(preds_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs4701]",
   "language": "python",
   "name": "conda-env-cs4701-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
